{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jurtta66/tracking_synapse/blob/main/basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYLRXINgxI_J"
      },
      "source": [
        "Используем fast.ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1DHURzXyHzM"
      },
      "source": [
        "Сетап"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jWu8owZexII3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf83eac-5638-450f-94c3-ee5d77166f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "! [ -e /content ] && pip install -Uqq fastbook nbdev\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "from fastbook import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_4jawb1EpGp"
      },
      "source": [
        "Сперва преобразуем кадры с видеокамер в картинки людей.\n",
        "Для этого пробежимся по кадрам, на каждом детектируем людей, для каждой детекции обрежем bbox из кадра в новое изображение.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD900ofUE84B"
      },
      "source": [
        "Путь к папке с кадрами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hLLBzVcyfSK"
      },
      "outputs": [],
      "source": [
        "path = Path('/content/drive/MyDrive/comp_v/data/images/train')\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-P8Kwh4EFGG4"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r5zXkIJFIsy"
      },
      "source": [
        "Скачаем предобученную модель для детекции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNOdgDlwFbIP"
      },
      "outputs": [],
      "source": [
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O8HVlBGFeYy"
      },
      "source": [
        "Определим функции predict и отрисовки картинки кадра с bbox, 'вырезки' области из изображения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug58kcEZFljq"
      },
      "outputs": [],
      "source": [
        "def predict(img):\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = transform(img)\n",
        "    with torch.no_grad():\n",
        "        pred = model([img])[0]\n",
        "    return pred['labels'].detach().numpy(), pred['boxes'].detach().numpy(), pred['scores'].detach().numpy()\n",
        "\n",
        "def show_img_with_bbox(img, bbox):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img)\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='g', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "def show_img(img):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "def crop_img(img, bbox):\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    return img.crop((x1, y1, x2, y2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVmk1KV8FrGW"
      },
      "source": [
        "Закинем в папку вырезанные изображения людей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25uU_2Uexryu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "13cf5111-8578-4b47-fd8d-d66db65762ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/cv/synapse/person_images'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "output_path = Path('/content/gdrive/MyDrive/cv/synapse/person_images')\n",
        "str(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O0-R4R2DYD8"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for img_path in path.ls():\n",
        "    person_count = 0\n",
        "    i += 1\n",
        "    print(i, end = ', ')\n",
        "    img = PILImage.create(img_path)\n",
        "    labels, boxes, scores = predict(img)\n",
        "    if i == 201:\n",
        "        print('done 200, input?')\n",
        "        input()\n",
        "    for label, bbox, score in zip(labels, boxes, scores):\n",
        "        if label == 1 and score > 0.5:\n",
        "            person_count += 1\n",
        "            #print(f'Person detected in {img_path.name} with probability {score:.04f}')\n",
        "            #print(f'Bounding box: {bbox}')\n",
        "            person = crop_img(img, bbox)\n",
        "            person.save(str(output_path) + '/' + str(i) + '_' + str(person_count) + '.jpg', 'JPEG')\n",
        "            #show_img_with_bbox(img, bbox)\n",
        "            #show_img(person)\n",
        "            #input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CixXHOZCobWY",
        "outputId": "bd87eee0-dd80-4156-d4a0-bce2c967b187"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2355"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "len(output_path.ls())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6QYoAyo1I3U"
      },
      "source": [
        "Теперь пропустим этих людей через resnet с обрезанным последним слоем, и для каждого человека сохраним вектор значений предпоследнего слоя. (если бы сохраняли последний слой реснет - то получили бы аргумент для сигмоиды -> вероятность, что на картинке человек или собака или допустим чайник. Нам это не нужно, мы знаем что на картинке человек.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmZ6Vi2MQBTu"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize((50, 150)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "transform_to_pil = T.ToPILImage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7jtH2XC7VGT"
      },
      "source": [
        "Загружаем обрезанные изображения людей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rhKHU2f7ZZS"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "for filename in os.listdir(str(output_path)):\n",
        "    image = Image.open(os.path.join(str(output_path), filename))\n",
        "    #show_img(image)\n",
        "    #print(image.size)\n",
        "    image = transform(image)\n",
        "\n",
        "    #show_img(transform_to_pil(image))\n",
        "    #print(transform_to_pil(image).size)\n",
        "    #input()\n",
        "    images.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbRnqFOqBHmj"
      },
      "outputs": [],
      "source": [
        "images[0].shape, len(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRKrwg3nA3MS"
      },
      "source": [
        "Теперь возьмем классификатор, обрежем выходной слой и пустим через него наши изображения, после чего возьмем значения нового выходного слоя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVCyDCl5z59"
      },
      "outputs": [],
      "source": [
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# Удаление последнего слоя\n",
        "modules = list(resnet.children())[:-1]\n",
        "resnet = torch.nn.Sequential(*modules)\n",
        "\n",
        "# Отключение обучения для всех слоев\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Пропуск изображений через модель\n",
        "features = []\n",
        "for image in images:\n",
        "    # Изменение размера изображения и добавление размерности пакета\n",
        "    image = torch.unsqueeze(image, 0)\n",
        "    # Пропуск изображения через модель\n",
        "    feature = resnet(image)\n",
        "    # Удаление размерности пакета и добавление в список признаков\n",
        "    features.append(torch.squeeze(feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CGfiPxxbMT7"
      },
      "source": [
        "Сделаем тоже самое, но для 2355 изображений людей и специальной сетки re-id OSNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xoDGYGEq74z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86b5c70-f4a5-40f0-8656-1372fb44c433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchreid\n",
            "  Downloading torchreid-0.2.5.tar.gz (92 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchreid\n",
            "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144328 sha256=84a95fed89a3197c6fb289ebc1003dcb0e6878203f4cc7744c0cf8f72cdfe47a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2d/36/816a48465cefd3e58be0317648a4c52ce39ae817f935212099\n",
            "Successfully built torchreid\n",
            "Installing collected packages: torchreid\n",
            "Successfully installed torchreid-0.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install torchreid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXOP6zRMqsBQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "15de78c3-3312-4a9d-b984-0a6a0a195a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY\n",
            "To: /root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\n",
            "100%|██████████| 10.9M/10.9M [00:00<00:00, 55.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
            "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nosnet_features = []\\nwith torch.no_grad():\\n    for image in images:\\n        # Изменение размера изображения и добавление размерности пакета\\n        image = torch.unsqueeze(image, 0)\\n        # Пропуск изображения через модель\\n        feature = osnet(image)\\n        # Удаление размерности пакета и добавление в список признаков\\n        osnet_features.append(torch.squeeze(feature))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torchreid\n",
        "\n",
        "# Загрузка OSNet\n",
        "osnet = torchreid.models.build_model(\n",
        "    name='osnet_x1_0',\n",
        "    num_classes=1041, # это значение не имеет значения для извлечения признаков\n",
        "    loss='softmax',\n",
        "    pretrained=True\n",
        ")\n",
        "\n",
        "# Перевод модели в режим оценки\n",
        "osnet.eval()\n",
        "\n",
        "# Пропуск изображений через модель\n",
        "'''\n",
        "osnet_features = []\n",
        "with torch.no_grad():\n",
        "    for image in images:\n",
        "        # Изменение размера изображения и добавление размерности пакета\n",
        "        image = torch.unsqueeze(image, 0)\n",
        "        # Пропуск изображения через модель\n",
        "        feature = osnet(image)\n",
        "        # Удаление размерности пакета и добавление в список признаков\n",
        "        osnet_features.append(torch.squeeze(feature))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSfdz_qgbEeX"
      },
      "outputs": [],
      "source": [
        "osnet_features_np = np.array([tensor.tolist() for tensor in osnet_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cghh-FY7Zltl"
      },
      "source": [
        "Сохраним osnet_features в текстовый файл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN2yduHfZlOm"
      },
      "outputs": [],
      "source": [
        "#np.save('/content/drive/MyDrive/cv/synapse/osnet-features-vectors', osnet_features_np)\n",
        "osnet_features_np = np.load('/content/gdrive/MyDrive/cv/synapse/osnet-features-vectors/osnet-features-vectors.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "osnet_features_np.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKmD5kPUPDCy",
        "outputId": "c6fa404f-57f9-4771-de7c-d7184adb4354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2355, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGRwU5hRbRJl"
      },
      "source": [
        "Сократим размерности векторов признаков с (1, 512) до (1, 2) и (1, 3) с помощью t-sne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haMGA8idb6LK"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# экземпляры t-sne\n",
        "tsne2 = TSNE(n_components=2)\n",
        "tsne3 = TSNE(n_components=3)\n",
        "\n",
        "# Преобразование t-sne\n",
        "osnet_2 = tsne2.fit_transform(osnet_features_np)\n",
        "osnet_3 = tsne3.fit_transform(osnet_features_np)\n",
        "#f_reduced_2 = tsne2.fit_transform(features_np)\n",
        "#f_reduced_3 = tsne3.fit_transform(features_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjzJJ7MYdy8N"
      },
      "source": [
        "Кластеризуем вектора размерности 2 (3). Я поглядел на первые 100 картинок, там 45- 60 людей. Хотя здесь 2355 людей, и если отношение такое же, нужна примерна половина кластеров, но я сделал 700. Так возможно похожие люди в один кластер попадут (подозреваю, стоит сделать еще меньше по этой же причине). А пока хочется именно одинакового человека на кластере отлавливать."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#n_clusters = 770  #\n",
        "n_clusters = 24\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "\n",
        "kmeans.fit_transform(osnet_clean_2)\n",
        "cluster_labels = kmeans.labels_\n",
        "cluster_labels"
      ],
      "metadata": {
        "id": "zYRNl3DzPUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cluster_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vpXw2G6U5h7",
        "outputId": "e16fbe1a-9062-4cc3-a0ef-e9107265f7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HRw6Ra9fIu0"
      },
      "source": [
        "Покажем на 2д графике кластеры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-shlyXpYfITx"
      },
      "outputs": [],
      "source": [
        "def plot_clusters(feature_vectors, n_clusters, cluster_labels=cluster_labels):\n",
        "    # Спектральная карта цветов\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, n_clusters))\n",
        "\n",
        "    # График\n",
        "    for i in range(len(feature_vectors)):\n",
        "        point = feature_vectors[i]\n",
        "        plt.scatter(point[0], point[1], c=colors[cluster_labels[i]])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь хотим посмотреть, насколько разрозненными получаются точки, соответствующие разным кластерам (насколько четко osnet плоттит людей). Для этого пробежимся по кластерам, найдем большие (size >= 3), запишем их в массив big_clusters.\n",
        "Все это - чтобы создать чистый датасет, с 25 людьми, по 3-5 кадров на каждого."
      ],
      "metadata": {
        "id": "h2Enl_pRp35a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def show_cluster_imgs(cluster_label, img_path, batch_size=10):\n",
        "    # Цикл по директории, получаем список индексов людей одного кластера.\n",
        "    cluster_indices = [i for i in range(len(osnet_2)) if cluster_labels[i] == cluster_label]\n",
        "    # Цикл по папке людей, если индекс есть в cluster_indices -> добавляем в массив\n",
        "    cluster_imgs = []\n",
        "    print(len(cluster_indices))\n",
        "    index = 0\n",
        "    for img in img_path.ls():\n",
        "        if index in cluster_indices:\n",
        "            img = PILImage.create(img)\n",
        "            #img = transform(img)\n",
        "            cluster_imgs.append(img)\n",
        "        index += 1\n",
        "    print(len(cluster_imgs))\n",
        "\n",
        "    for i in range(min(batch_size, len(cluster_imgs))):\n",
        "        show_img(cluster_imgs[i])\n",
        "\n",
        "#show_cluster_imgs(106, output_path)\n",
        "'''\n",
        "def calc_cluster(cluster_label, img_path):\n",
        "    # out: python список pil potential изображений людей, принадлежащих одному кластеру\n",
        "\n",
        "    # Цикл по директории, получаем список индексов людей одного кластера.\n",
        "    cluster_indices = [i for i in range(len(osnet_2)) if cluster_labels[i] == cluster_label]\n",
        "\n",
        "    # Цикл по папке людей, если индекс есть в cluster_indices -> добавляем в массив\n",
        "    cluster_imgs = []\n",
        "    index = 0\n",
        "    for img in img_path.ls():\n",
        "        if index in cluster_indices:\n",
        "            img = PILImage.create(img)\n",
        "            #img = transform(img)\n",
        "            cluster_imgs.append(img)\n",
        "        index += 1\n",
        "    return cluster_imgs\n",
        "\n",
        "def show_cluster(cluster_imgs, batch_size=10):\n",
        "    # Показываем картинки одного кластера\n",
        "    for i in range(min(batch_size, len(cluster_imgs))):\n",
        "        img = PILImage.create(cluster_imgs[i])\n",
        "        show_img(img)\n",
        "\n",
        "def calc_big_clusters(bar=3, big_clusters_num=25):\n",
        "    big_clusters = []  # Сюда записываем кластеры размера >=bar\n",
        "    # Пробегаемся по меткам кластеров и добавляем большие кластера в список\n",
        "    for i in range(n_clusters):\n",
        "        cluster = calc_cluster(cluster_label=i, img_path=output_path)\n",
        "        if len(cluster) >= bar:\n",
        "            big_clusters.append((i, cluster))\n",
        "        # Если наелись\n",
        "        if len(big_clusters) == big_clusters_num:\n",
        "            return big_clusters\n",
        "    return ['did not reach num']\n",
        "\n",
        "#big_clusters = calc_big_clusters()\n",
        "#big_clusters_np = np.array(big_clusters)\n",
        "#big_clusters_np.shape"
      ],
      "metadata": {
        "id": "LnpCcSoLQhLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_clusters_np = np.array([cluster[1] for cluster in big_clusters])\n",
        "[len(cluster) for cluster in big_clusters_np]"
      ],
      "metadata": {
        "id": "9kTXRc3GvQAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим на достоверность: покажем картинки людей с одного ббольшого кластера"
      ],
      "metadata": {
        "id": "z3C-meUhwXFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in big_clusters:\n",
        "    print(f\"cluster number {cluster[0]}\")\n",
        "    print(cluster[1])\n",
        "    show_cluster(cluster[1])\n",
        "    #for img in big_clusters[1]:\n",
        "        #print(img)\n",
        "        #show_img(img)\n",
        "    input()"
      ],
      "metadata": {
        "id": "STrjp5HbwWQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я поглядел, из 25 кластеров один поломанный, штуки 4 не очень качественные. Прогоним людей из кластеров через osnet."
      ],
      "metadata": {
        "id": "XeG6QgVU05nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_clusters_indices = [7]  # поломанный кластер\n",
        "big_clusters = [pair for pair in big_clusters if pair[0] not in noisy_clusters_indices]"
      ],
      "metadata": {
        "id": "P_OnqOJ3CZ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "osnet_clean = []  # векторы признаков для новых изобраежений\n",
        "with torch.no_grad():\n",
        "    i =0\n",
        "    # перебираем кластеры\n",
        "    for cluster in big_clusters:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        label, imgs = cluster\n",
        "        for img in imgs:\n",
        "            img_tensor = transform(PILImage.create(img))\n",
        "            img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "            vector = osnet(img_tensor)\n",
        "            osnet_clean.append(torch.squeeze(vector))"
      ],
      "metadata": {
        "id": "DH6hbUfr1yEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "osnet_clean_np = np.array([tensor.tolist() for tensor in osnet_clean])\n",
        "osnet_clean_np.shape\n",
        "#osnet_clean_np[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMcj33Sy4feA",
        "outputId": "8e1142b2-5179-42a0-c284-ebb54e7bf522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование t-sne\n",
        "osnet_clean_2 = tsne2.fit_transform(osnet_clean_np)"
      ],
      "metadata": {
        "id": "75vaxjww5Ojj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим график. Для 25 людей получается не прям плохая картинка."
      ],
      "metadata": {
        "id": "S3Wcqwgb5bMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#osnet_clean_2[0][1]\n",
        "plot_clusters(feature_vectors=osnet_clean_2, n_clusters=24)"
      ],
      "metadata": {
        "id": "TU1SCPnj50fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем значение метрики map@K для алгоритма Re-id на основе евкл растояния. Для этого используем датасет с помеченными людьми."
      ],
      "metadata": {
        "id": "TGBXHqxkyQik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка изображений"
      ],
      "metadata": {
        "id": "vCuI5HfvZ0EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_people_path = Path('/content/gdrive/MyDrive/cv/synapse/whole_body_images')"
      ],
      "metadata": {
        "id": "ZN2YAlTsyUW2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_id(filename):\n",
        "    return int(filename[:3])"
      ],
      "metadata": {
        "id": "AXL70QhJacN8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_images = []\n",
        "labels = []\n",
        "# Проходимся по директории\n",
        "for img_path in labeled_people_path.ls():\n",
        "    labels.append(get_id(img_path.name))\n",
        "    img = PILImage.create(img_path)\n",
        "    #img = transform(img)\n",
        "    labeled_images.append(img)"
      ],
      "metadata": {
        "id": "1c8KsEQsaFv1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_images[200].size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpq6PyfplqUc",
        "outputId": "60a2c709-9cdb-46e7-caed-dfdd4788a411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47, 156)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь пропустим изображения через osnet"
      ],
      "metadata": {
        "id": "ia_p4ufpdjJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "osnet_mapk = []\n",
        "i = 0\n",
        "for img in labeled_images:\n",
        "    img_tensor = torch.unsqueeze(transform(img), 0)\n",
        "    vector = osnet(img_tensor)\n",
        "    osnet_mapk.append(torch.squeeze(vector))"
      ],
      "metadata": {
        "id": "V4qhtHXQdbTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "сохраним/ загрузим osnet_mapk"
      ],
      "metadata": {
        "id": "6wmSb-tcmJGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#osnet_mapk_np = np.array([tensor.tolist() for tensor in osnet_mapk])\n",
        "#np.save('/content/gdrive/MyDrive/cv/synapse/osnet-mapk', osnet_mapk_np)\n",
        "osnet_mapk = np.load('/content/gdrive/MyDrive/cv/synapse/osnet-mapk.npy')"
      ],
      "metadata": {
        "id": "3YXlPopFmLLQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "алгоритм re-id"
      ],
      "metadata": {
        "id": "hSkWmKXtfXP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reid_euclid(v, vectors, n=5):\n",
        "    # in: вектор, список векторов (вкл этот вектор)\n",
        "    # out: первые n самых близких по евклидовой метрике векторов к v\n",
        "\n",
        "    distances = [np.linalg.norm(np.array(v) - np.array(vector)) for vector in vectors]\n",
        "    closest_indices = np.argsort(distances)[1:n+1]\n",
        "    return closest_indices.tolist()\n",
        "\n",
        "reid_euclid([0, 0], [[0, 0], [1, 1], [2, 2], [0, 1], [-1, 19]], n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDnWj9fLfZwi",
        "outputId": "be567085-a69a-4dbd-a1d4-2452bb551de0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Организуем вычисление map@k для нашего проаннотированного датасета."
      ],
      "metadata": {
        "id": "g3pV71OvrC5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pk(img_index, objs, r_true, K=5):\n",
        "    # in: индекс изображения, по которому вычисляем pk, изображения, аннотации, K.\n",
        "    # out: значение p@k\n",
        "    preds = reid_euclid(objs[img_index], objs, n=K)\n",
        "    #print(preds)\n",
        "    return sum([r_true[img_index]==r_true[i] for i in preds])/K\n",
        "    #return preds\n",
        "    #return sum([r_true[img_index] == reid_euclid(objs[img_index], objs, n=5)[img_index] for img_index in range(len(objs))])/K\n",
        "\n",
        "#for index in [2, 114, 212, 192, 284]:\n",
        "    #labeled_images[index].show()\n"
      ],
      "metadata": {
        "id": "4Ejw0kxVoGKb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(img_index, objs, r_true, K=5):\n",
        "    preds = reid_euclid(objs[img_index], objs, n=K)\n",
        "    weighted_pk = [(r_true[img_index]==r_true[preds[i]]) * pk(img_index, objs, r_true, i+1) for i in range(len(preds))]\n",
        "    return sum(weighted_pk)/K\n",
        "\n",
        "# тест\n",
        "test_arr = [[0, 1], [3, 1], [2, 0], [3, 3], [1, 1], [0, 0.5], [5, 3], [3, 2]]\n",
        "test_labels = [0, 1, 0, 2, 0, 0, 2, 1]\n",
        "apk(1, test_arr, test_labels, K=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikuPHmVYpsY7",
        "outputId": "c1c90a95-fb44-41d2-c79b-e286fece8da9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mapk(objs, r_true, K=5):\n",
        "    N = len(objs)\n",
        "    apk_sum = 0\n",
        "    for img_index in range(N):\n",
        "        apk_sum += apk(img_index, objs, r_true, K)\n",
        "    return (apk_sum)/N\n",
        "#mapk(test_arr, test_labels, K=3)\n",
        "mapk(osnet_mapk, labels, K=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_xn_UHP9_8T",
        "outputId": "aebfd03d-c720-4640-a5d8-1dc5148863b7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3124000000000003"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "в датасете было по 5 фоток каждого человека. map@k получился плоховатый, возможно что-то в алгоритме напутал (при написании тестировал на простых векторах, вроде работало). map@k макс при K=5, то есть условно у алгоритма есть 5 взвешенных попыток угадать 4 фото из датасета, (не считая самого рассматриваемого изображения)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYoGMOYc_34F"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J6cwivMrFGq-PilQt6pOa-3dzHH0FSkt",
      "authorship_tag": "ABX9TyNx7OGc4fBSTZNHtRJaIFj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}